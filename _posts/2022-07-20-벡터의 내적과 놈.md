---
published : true 
title : 벡터의 내적과 놈  
layout : single 
toc : true 
katex : True 
---
# 벡터의 내적과 놈

벡터의 내적과 놈의 의의와 계산 방법을 배웁니다. 벡터의 조작에 익숙해집시다.

## 내적

내적은 벡터끼리의 곱의 한 종류인데 다음과 같이 각 요소끼리 곱한 값을 총합해서 정의합니다.

$$
\vec{a} = (a_1,a_2,\cdots,a_n)
\vec{b} = (b_1,b_2,\cdots,b_n)
$$

위와 같을 때, $$\vec{a} . \vec{b}$$ 로 표시되게 하면 다음과 같습니다.

$$
\vec{a} \cdot \vec{b} = (a_1,a_2,\cdots,a_n) \cdot (b_1,b_2,\cdots,b_n)\\
= (a_1b_1 + a_2b_2 + \cdot + a_nb_n) \\
= \sum_{k=1}^{n}{a_kb_k}
$$

내적을 구할 때는 두 개의 벡터의 요소 수가 같아야 합니다. 내적은 삼각함수를 사용해 구하는 방법이 있는데 이것에 대해선 나중에 설명합니다.

## 내적의 구현

내적은 numpy의 **dot()** 함수로 간단하게 구할 수 있습니다. 또한 **sum()** 함수를 사용해서 각 요소의 곱의 총합으로도 구할 수도 있습니다.

양쪽을 비교해 봅시다.




```python
import numpy as np

a = np.array([1,2,3])
b = np.array([3,2,1])

print("-- dot() 함수 --")
print(np.dot(a,b)) # dot() 함수에 의한 내적
print("--- 곱의 총합 --") #
print(np.sum(a*b)) # 곱의 총합에 의한 내적
```

    -- dot() 함수 --
    10
    --- 곱의 총합 --
    10


**dot()** 함수, 곱의 총합 모두 같은 결과가 나왔습니다. 내적은 예를 들어 두 개 벡터의 상관관계를 구할 때 등에 사용합니다. 

## 놈

놈은 벡터의 "크기"를 나타내는 양입니다. 인공지능에서 자주 쓰이는 놈으로는 $$L^2$$놈과 $$L^1$$놈이 있습니다.

### $$L^2$$놈

$$L^2$$ 놈은 다음과 같이 $$ || \vec{x}||{_2} $$로 나타냅니다. 벡터의 각 요소를 제곱하여 제곱근을 구해 계산합니다.

$$
|| \vec{x}||{_2} = \sqrt{x_1^2 + x_2^2 + \cdot + x_n^2} \\
= \sqrt{\sum_{k=1}^{n}{x_k^2}}
$$

### $$L^1$$ 놈


$$L^1$$ 놈은 다음과 같이 

$$ 
||\vec{x}||{_1} 
$$ 
로 나타냅니다. 벡터의 각 요소의 절대값을 더해서 계산합니다.



$$
\vec x_1 = |x_1| + |x_2| + ...  + |x_n| \\
= \sum_{k=1}^n x_k
$$



### 일반화된 놈

놈을 더욱 일반화한 $$L^p$$놈은 다음과 같이 나타냅니다

$$
||\vec{x}||_p = (x_1^p + x_2^p + \cdot + x_n^p)^\frac{1}{p} \\
= (\sum_{k=1}^{n}{x_k^p})^\frac{1}{p}
$$

놈에는 몇 가지 표시법이 있는데 인공지능에서는 이것들을 필요에 따라 구분해서 사용합니다.

### 놈의 구현

놈은 numpy의 **linalg.norm()** 함수를 이용해서 구할 수 있습니다.



```python
import numpy as np

a = np.array([1,1,-1,-1])

print("--L2놈--")
print(np.linalg.norm(a)) # L2놈 (디폴트)
print("--L1놈--") 
print(np.linalg.norm(a,1)) # L1놈


```

    --L2놈--
    2.0
    --L1놈--
    4.0


이상과 같이, 놈의 종류에 따라 벡터의 "크기"는 다른 값이 됩니다.
높은 인공지능에서 **정칙화**에 쓰입니다. 정칙화란 필요 이상으로 네트워크 합슥이 진행되는 것을 파라미터를 조절해서 예방하는 것입니다.
